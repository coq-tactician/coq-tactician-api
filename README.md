# Reinforcement learning for Tactician

## Coq plugin installation

The recommended `opam` version is `>= 2.1.0`. Other versions might work as well, but you may have to install some dependencies manually.

Notice: the installation depends on ocaml version 4.11.2 that is in conflict with glibc version >= 2.34
and therefore fails on Ubuntu 21.10. 


Notice: with current limitation of pin-depends and pinned relative path it is strictly necessary to execute
`opam install ./coq-tactician-reinforce.opam.locked --yes` in the below script from the directory of the opam file.


```
opam switch create my-switch --empty
opam repo add coq-released https://coq.inria.fr/opam/released
opam repo add coq-extra-dev https://coq.inria.fr/opam/extra-dev
opam repo add coq-core-dev https://coq.inria.fr/opam/core-dev
opam repo add custom-archive https://github.com/LasseBlaauwbroek/custom-archive.git
git clone --recurse-submodules git@github.com:coq-tactician/coq-tactician-reinforce.git 
cd coq-tactician-reinforce
opam install ./coq-tactician-reinforce.opam.locked --yes
cp $(opam var prefix)/.opam-switch/build/coq-tactician-reinforce.~dev/config $(opam var coq-tactician:etc)/injection-flags
```
If you encounter problems while installing the `lwt` dependency, try installing `opam install conf-libev`.

Optional but recommended additional software: `graphviz` (install through your distribution's package manager)




## Containers

To verify installation in a controlled enviroment we provide Dockerfile script. The Dockerfile can be used with `docker` or `podman`. To build the image, run from the directory containing the Dockerfile
```
podman build -t tac:test . 
```
We recommend using podman in rootless mode as there are certain unresolved limitation of bubblewrap / user namespaces with docker container.  

Notice: as of `podman 3.3.1` and `opam 2.0.5` sometimes race conditions have been detected with building on multicore. Updating podman and/or opam might fix the issue.

## Available Commands

These commands will create a graph of some object, and write it to `graph.pdf` (if `graphviz` is available).

The following commands are always available:
```
[Full] [LGraph | Graph | DAG] Ident identifier.
[Full] [LGraph | Graph | DAG] Term term.
```
Normally, the commands print a non-transitive graph. The `[Full]` modifier changes this so that the full transitive graph of definitions is added.

Additionally, in proof mode, these commands are available:
```
[Full] [LGraph | Graph | DAG] Proof.
```

Options that modify the graphs generated by the commands above are
```
[Set | Unset] Tactician Reinforce Visualize Ordered.
[Set | Unset] Tactician Reinforce Visualize Labels.
```

## Reinforcement learning

Finally, the command `Reinforce.` will initiate a reinforcement session. An example of this is available in
[theories/ReinforceTest.v](theories/ReinforceTest.v).
To do this, you need to have a python client running. An example is available in [python/fake_reinforcement_client.py](python/fake_reinforcement_client.py).
You run it from the root of the repository as `python3 python/fake_reinforcement_client.py`.
Everybody is invited to make this a proper python package. Until then, to run the python code, you need the following packages:
```
pip intall graphviz
pip install pycapnp
pip install ptpython
```

If you run
[python/fake_reinforcement_client.py --interactive](python/fake_reinforcement_client.py) then an innteractive shell appears where you can
manually interact with the environment. Whenever a tactic is executed,
the resulting proof state if visualized in the file
`python_graph.pdf`.
