# Reinforcement learning for Tactician

## Assumptions

The recommended `opam` version is `>= 2.1.0`. Other versions might work as well, but you may have to install some dependencies manually.

Notice: the installation depends on ocaml version 4.11.2 that is in conflict with glibc version >= 2.34
and therefore fails on Ubuntu 21.10. 

On Ubuntu 20.04 the packages installed by the following are required:

```
sudo apt-get --yes install graphviz capnproto libcapnp-dev pkg-config libev-dev
```

## Installation



Notice: with current limitation of pin-depends and pinned relative path it is strictly necessary to execute
`opam install ./coq-tactician-reinforce.opam --yes` in the below script from the directory of the opam file.

```
opam switch create tactician-reinforce --empty
opam pin coq-tactician-reinforce.opam git+ssh://git@github.com/coq-tactician/coq-tactician-reinforce.git --yes
```
If you encounter problems while installing the `lwt` dependency, try installing `opam install conf-libev`.

Optional but recommended additional software: `graphviz` (install through your distribution's package manager)


## For developers: 

After an update of a commit of a submodule `coq-tactician` run `make` to generate the 
`coq-tactician-reinforce.opam` that pins this commit apropriately.


## Containers

To verify installation in a controlled enviroment we provide Dockerfile script. The Dockerfile can be used with `docker` or `podman`. To build the image, run from the directory containing the Dockerfile
```
podman build -t tac:test . 
```
We recommend using podman in rootless mode as there are certain unresolved limitation of bubblewrap / user namespaces with docker container.  

Notice: as of `podman 3.3.1` and `opam 2.0.5` sometimes race conditions have been detected with building on multicore. Updating podman and/or opam might fix the issue.

## Available Commands

These commands will create a graph of some object, and write it to `graph.pdf` (if `graphviz` is available).

The following commands are always available:
```
[Full] [LGraph | Graph | DAG] Ident identifier.
[Full] [LGraph | Graph | DAG] Term term.
```
Normally, the commands print a non-transitive graph. The `[Full]` modifier changes this so that the full transitive graph of definitions is added.

Additionally, in proof mode, these commands are available:
```
[Full] [LGraph | Graph | DAG] Proof.
```

Options that modify the graphs generated by the commands above are
```
[Set | Unset] Tactician Reinforce Visualize Ordered.
[Set | Unset] Tactician Reinforce Visualize Labels.
```

## Reinforcement learning

Finally, the command `Reinforce.` will initiate a reinforcement session. An example of this is available in
[theories/ReinforceTest.v](theories/ReinforceTest.v).
To do this, you need to have a python client running. An example is available in [python/fake_reinforcement_client.py](python/fake_reinforcement_client.py).
You run it from the root of the repository as `python3 python/fake_reinforcement_client.py`.
Everybody is invited to make this a proper python package. Until then, to run the python code, you need the following packages:
```
pip intall graphviz
pip install pycapnp
pip install ptpython
```

If you run
[python/fake_reinforcement_client.py --interactive](python/fake_reinforcement_client.py) then an innteractive shell appears where you can
manually interact with the environment. Whenever a tactic is executed,
the resulting proof state if visualized in the file
`python_graph.pdf`.
